There are many methods and tools to communicate in today's world. However, the most effective way to communicate is to speak. How important acoustic information is still visible by everyone. However, in these communication steps, acoustic information may not always be transmitted properly to the receiver. The main purpose of this project is to make predictions on the acoustic information by using visual information in the videos where such situations occur. So, it's intended to predict words that people who don't have any acoustic components in the video just say from their mouth and lip movements. 
A number of algorithms were used to make these predictions. The system consists of a combination of these algorithms, which are completely different tasks and functions. First, the system receives the visual input as the video by the user. It captures the captured video from the video at certain time intervals by looking at its properties such as size, duration, resolution. However, these images are not full images that have been cut off from the video. These images are only images that show the mouth and its environment. These saved images are merged in the background and saved as a larger image file. This system, which was previously trained with a large dataset with machine learning, examines this large image and begins to compare it. Finally, after this comparison phase, the system tries to guess the word that is said in this input video depending on the human lip movements. Returns the highest probability word output, although there are many results in estimates. All the studies and algorithms were realized in Anaconda and Python environment. Lip reading was examined in detail by learning the machine based on basic ways such as lip detection, lip tracking.
